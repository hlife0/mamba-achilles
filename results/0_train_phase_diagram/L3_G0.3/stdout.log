Configuration saved to results/0_train_phase_diagram/L3_G0.3/config.json
Using device: cuda
Creating datasets...
Train dataset size: 300000
Eval dataset size: 1800
Creating model: MambaForComposite
Model parameters: 97,728

Starting training for 200 epochs...
Epoch 1/200 | Loss: 5.5738 | LR: 0.000010
Epoch 2/200 | Loss: 5.5559 | LR: 0.000034
Epoch 3/200 | Loss: 5.5208 | LR: 0.000058
Epoch 4/200 | Loss: 5.4750 | LR: 0.000082
Epoch 5/200 | Loss: 5.4199 | LR: 0.000106
Epoch 6/200 | Loss: 5.3622 | LR: 0.000130
Epoch 7/200 | Loss: 5.2957 | LR: 0.000154
Epoch 8/200 | Loss: 5.2263 | LR: 0.000178
Epoch 9/200 | Loss: 5.1591 | LR: 0.000202
Epoch 10/200 | Loss: 5.0833 | Comp Acc: 0.0089 (best: 0.0089@10) | Sym Acc: 0.0128 (best: 0.0128@10) | LR: 0.000226
Epoch 11/200 | Loss: 5.0102 | LR: 0.000250
Epoch 12/200 | Loss: 4.9350 | LR: 0.000250
Epoch 13/200 | Loss: 4.8543 | LR: 0.000250
Epoch 14/200 | Loss: 4.7817 | LR: 0.000250
Epoch 15/200 | Loss: 4.7168 | LR: 0.000250
Epoch 16/200 | Loss: 4.6572 | LR: 0.000250
Epoch 17/200 | Loss: 4.6089 | LR: 0.000249
Epoch 18/200 | Loss: 4.5718 | LR: 0.000249
Epoch 19/200 | Loss: 4.5478 | LR: 0.000249
Epoch 20/200 | Loss: 4.5339 | Comp Acc: 0.0122 (best: 0.0122@20) | Sym Acc: 0.0056 (best: 0.0128@10) | LR: 0.000249
Epoch 21/200 | Loss: 4.5259 | LR: 0.000248
Epoch 22/200 | Loss: 4.5211 | LR: 0.000248
Epoch 23/200 | Loss: 4.5178 | LR: 0.000248
Epoch 24/200 | Loss: 4.5160 | LR: 0.000247
Epoch 25/200 | Loss: 4.5146 | LR: 0.000247
Epoch 26/200 | Loss: 4.5137 | LR: 0.000246
Epoch 27/200 | Loss: 4.5126 | LR: 0.000246
Epoch 28/200 | Loss: 4.5119 | LR: 0.000245
Epoch 29/200 | Loss: 4.5112 | LR: 0.000245
Epoch 30/200 | Loss: 4.5105 | Comp Acc: 0.0133 (best: 0.0133@30) | Sym Acc: 0.0044 (best: 0.0128@10) | LR: 0.000244
Epoch 31/200 | Loss: 4.5101 | LR: 0.000243
Epoch 32/200 | Loss: 4.5094 | LR: 0.000243
Epoch 33/200 | Loss: 4.5089 | LR: 0.000242
Epoch 34/200 | Loss: 4.5084 | LR: 0.000241
Epoch 35/200 | Loss: 4.5077 | LR: 0.000241
Epoch 36/200 | Loss: 4.5074 | LR: 0.000240
Epoch 37/200 | Loss: 4.5070 | LR: 0.000239
Epoch 38/200 | Loss: 4.5059 | LR: 0.000238
Epoch 39/200 | Loss: 4.5058 | LR: 0.000237
Epoch 40/200 | Loss: 4.5050 | Comp Acc: 0.0144 (best: 0.0144@40) | Sym Acc: 0.0083 (best: 0.0128@10) | LR: 0.000236
Epoch 41/200 | Loss: 4.5040 | LR: 0.000236
Epoch 42/200 | Loss: 4.5026 | LR: 0.000235
Epoch 43/200 | Loss: 4.5012 | LR: 0.000234
Epoch 44/200 | Loss: 4.4998 | LR: 0.000233
Epoch 45/200 | Loss: 4.4977 | LR: 0.000232
Epoch 46/200 | Loss: 4.4950 | LR: 0.000230
Epoch 47/200 | Loss: 4.4911 | LR: 0.000229
Epoch 48/200 | Loss: 4.4849 | LR: 0.000228
Epoch 49/200 | Loss: 4.4760 | LR: 0.000227
Epoch 50/200 | Loss: 4.4630 | Comp Acc: 0.0128 (best: 0.0144@40) | Sym Acc: 0.0167 (best: 0.0167@50) | LR: 0.000226
Epoch 51/200 | Loss: 4.4430 | LR: 0.000225
Epoch 52/200 | Loss: 4.4130 | LR: 0.000223
Epoch 53/200 | Loss: 4.3618 | LR: 0.000222
Epoch 54/200 | Loss: 4.2747 | LR: 0.000221
Epoch 55/200 | Loss: 4.1411 | LR: 0.000220
Epoch 56/200 | Loss: 3.9636 | LR: 0.000218
Epoch 57/200 | Loss: 3.7339 | LR: 0.000217
Epoch 58/200 | Loss: 3.4980 | LR: 0.000216
Epoch 59/200 | Loss: 3.2943 | LR: 0.000214
Epoch 60/200 | Loss: 3.0933 | Comp Acc: 0.0222 (best: 0.0222@60) | Sym Acc: 0.0517 (best: 0.0517@60) | LR: 0.000213
Epoch 61/200 | Loss: 2.8835 | LR: 0.000211
Epoch 62/200 | Loss: 2.6694 | LR: 0.000210
Epoch 63/200 | Loss: 2.4529 | LR: 0.000208
Epoch 64/200 | Loss: 2.2347 | LR: 0.000207
Epoch 65/200 | Loss: 2.0305 | LR: 0.000205
Epoch 66/200 | Loss: 1.8453 | LR: 0.000204
Epoch 67/200 | Loss: 1.6683 | LR: 0.000202
Epoch 68/200 | Loss: 1.4964 | LR: 0.000201
Epoch 69/200 | Loss: 1.3452 | LR: 0.000199
Epoch 70/200 | Loss: 1.2126 | Comp Acc: 0.0539 (best: 0.0539@70) | Sym Acc: 0.0356 (best: 0.0517@60) | LR: 0.000197
Epoch 71/200 | Loss: 1.0930 | LR: 0.000196
Epoch 72/200 | Loss: 0.9828 | LR: 0.000194
Epoch 73/200 | Loss: 0.8831 | LR: 0.000192
Epoch 74/200 | Loss: 0.7970 | LR: 0.000191
Epoch 75/200 | Loss: 0.7165 | LR: 0.000189
Epoch 76/200 | Loss: 0.6469 | LR: 0.000187
Epoch 77/200 | Loss: 0.5821 | LR: 0.000185
Epoch 78/200 | Loss: 0.5223 | LR: 0.000184
Epoch 79/200 | Loss: 0.4712 | LR: 0.000182
Epoch 80/200 | Loss: 0.4247 | Comp Acc: 0.0300 (best: 0.0539@70) | Sym Acc: 0.0350 (best: 0.0517@60) | LR: 0.000180
Epoch 81/200 | Loss: 0.3822 | LR: 0.000178
Epoch 82/200 | Loss: 0.3435 | LR: 0.000176
Epoch 83/200 | Loss: 0.3093 | LR: 0.000175
Epoch 84/200 | Loss: 0.2788 | LR: 0.000173
Epoch 85/200 | Loss: 0.2520 | LR: 0.000171
Epoch 86/200 | Loss: 0.2278 | LR: 0.000169
Epoch 87/200 | Loss: 0.2069 | LR: 0.000167
Epoch 88/200 | Loss: 0.1866 | LR: 0.000165
Epoch 89/200 | Loss: 0.1679 | LR: 0.000163
Epoch 90/200 | Loss: 0.1524 | Comp Acc: 0.0239 (best: 0.0539@70) | Sym Acc: 0.0167 (best: 0.0517@60) | LR: 0.000161
Epoch 91/200 | Loss: 0.1378 | LR: 0.000159
Epoch 92/200 | Loss: 0.1246 | LR: 0.000158
Epoch 93/200 | Loss: 0.1128 | LR: 0.000156
Epoch 94/200 | Loss: 0.1030 | LR: 0.000154
Epoch 95/200 | Loss: 0.0948 | LR: 0.000152
Epoch 96/200 | Loss: 0.0863 | LR: 0.000150
Epoch 97/200 | Loss: 0.0791 | LR: 0.000148
Epoch 98/200 | Loss: 0.0724 | LR: 0.000146
Epoch 99/200 | Loss: 0.0667 | LR: 0.000144
Epoch 100/200 | Loss: 0.0622 | Comp Acc: 0.0211 (best: 0.0539@70) | Sym Acc: 0.0167 (best: 0.0517@60) | LR: 0.000142
Epoch 101/200 | Loss: 0.0572 | LR: 0.000140
Epoch 102/200 | Loss: 0.0530 | LR: 0.000138
Epoch 103/200 | Loss: 0.0494 | LR: 0.000136
Epoch 104/200 | Loss: 0.0461 | LR: 0.000134
Epoch 105/200 | Loss: 0.0435 | LR: 0.000132
Epoch 106/200 | Loss: 0.0408 | LR: 0.000130
Epoch 107/200 | Loss: 0.0384 | LR: 0.000128
Epoch 108/200 | Loss: 0.0362 | LR: 0.000126
Epoch 109/200 | Loss: 0.0341 | LR: 0.000124
Epoch 110/200 | Loss: 0.0327 | Comp Acc: 0.0239 (best: 0.0539@70) | Sym Acc: 0.0156 (best: 0.0517@60) | LR: 0.000122
Epoch 111/200 | Loss: 0.0309 | LR: 0.000120
Epoch 112/200 | Loss: 0.0293 | LR: 0.000118
Epoch 113/200 | Loss: 0.0278 | LR: 0.000116
Epoch 114/200 | Loss: 0.0266 | LR: 0.000114
Epoch 115/200 | Loss: 0.0255 | LR: 0.000112
Epoch 116/200 | Loss: 0.0246 | LR: 0.000110
Epoch 117/200 | Loss: 0.0233 | LR: 0.000108
Epoch 118/200 | Loss: 0.0223 | LR: 0.000106
Epoch 119/200 | Loss: 0.0215 | LR: 0.000104
Epoch 120/200 | Loss: 0.0208 | Comp Acc: 0.0217 (best: 0.0539@70) | Sym Acc: 0.0189 (best: 0.0517@60) | LR: 0.000102
Epoch 121/200 | Loss: 0.0202 | LR: 0.000101
Epoch 122/200 | Loss: 0.0195 | LR: 0.000099
Epoch 123/200 | Loss: 0.0187 | LR: 0.000097
Epoch 124/200 | Loss: 0.0181 | LR: 0.000095
Epoch 125/200 | Loss: 0.0175 | LR: 0.000093
Epoch 126/200 | Loss: 0.0168 | LR: 0.000091
Epoch 127/200 | Loss: 0.0164 | LR: 0.000089
Epoch 128/200 | Loss: 0.0158 | LR: 0.000087
Epoch 129/200 | Loss: 0.0155 | LR: 0.000085
Epoch 130/200 | Loss: 0.0149 | Comp Acc: 0.0217 (best: 0.0539@70) | Sym Acc: 0.0200 (best: 0.0517@60) | LR: 0.000084
Epoch 131/200 | Loss: 0.0145 | LR: 0.000082
Epoch 132/200 | Loss: 0.0142 | LR: 0.000080
Epoch 133/200 | Loss: 0.0137 | LR: 0.000078
Epoch 134/200 | Loss: 0.0133 | LR: 0.000076
Epoch 135/200 | Loss: 0.0130 | LR: 0.000075
Epoch 136/200 | Loss: 0.0127 | LR: 0.000073
Epoch 137/200 | Loss: 0.0124 | LR: 0.000071
Epoch 138/200 | Loss: 0.0122 | LR: 0.000069
Epoch 139/200 | Loss: 0.0119 | LR: 0.000068
Epoch 140/200 | Loss: 0.0115 | Comp Acc: 0.0211 (best: 0.0539@70) | Sym Acc: 0.0267 (best: 0.0517@60) | LR: 0.000066
Epoch 141/200 | Loss: 0.0112 | LR: 0.000064
Epoch 142/200 | Loss: 0.0110 | LR: 0.000063
Epoch 143/200 | Loss: 0.0108 | LR: 0.000061
Epoch 144/200 | Loss: 0.0105 | LR: 0.000059
Epoch 145/200 | Loss: 0.0104 | LR: 0.000058
Epoch 146/200 | Loss: 0.0102 | LR: 0.000056
Epoch 147/200 | Loss: 0.0099 | LR: 0.000055
Epoch 148/200 | Loss: 0.0098 | LR: 0.000053
Epoch 149/200 | Loss: 0.0096 | LR: 0.000052
Epoch 150/200 | Loss: 0.0094 | Comp Acc: 0.0206 (best: 0.0539@70) | Sym Acc: 0.0228 (best: 0.0517@60) | LR: 0.000050
Epoch 151/200 | Loss: 0.0092 | LR: 0.000049
Epoch 152/200 | Loss: 0.0090 | LR: 0.000047
Epoch 153/200 | Loss: 0.0089 | LR: 0.000046
Epoch 154/200 | Loss: 0.0087 | LR: 0.000044
Epoch 155/200 | Loss: 0.0087 | LR: 0.000043
Epoch 156/200 | Loss: 0.0085 | LR: 0.000042
Epoch 157/200 | Loss: 0.0083 | LR: 0.000040
Epoch 158/200 | Loss: 0.0082 | LR: 0.000039
Epoch 159/200 | Loss: 0.0081 | LR: 0.000038
Epoch 160/200 | Loss: 0.0080 | Comp Acc: 0.0156 (best: 0.0539@70) | Sym Acc: 0.0167 (best: 0.0517@60) | LR: 0.000037
Epoch 161/200 | Loss: 0.0079 | LR: 0.000035
Epoch 162/200 | Loss: 0.0077 | LR: 0.000034
Epoch 163/200 | Loss: 0.0076 | LR: 0.000033
Epoch 164/200 | Loss: 0.0076 | LR: 0.000032
Epoch 165/200 | Loss: 0.0074 | LR: 0.000031
Epoch 166/200 | Loss: 0.0073 | LR: 0.000030
Epoch 167/200 | Loss: 0.0073 | LR: 0.000028
Epoch 168/200 | Loss: 0.0072 | LR: 0.000027
Epoch 169/200 | Loss: 0.0071 | LR: 0.000026

Early stopping at epoch 170: both metrics declined for 3 consecutive evaluations.

Training finished at epoch 170.
Best Composite Accuracy: 0.0539 (epoch 70)
Best Symmetric Accuracy: 0.0517 (epoch 60)
Metrics saved to results/0_train_phase_diagram/L3_G0.3/metrics.json

Training completed successfully!
