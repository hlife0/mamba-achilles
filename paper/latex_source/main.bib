@article{child2019generating,
  title={Generating long sequences with sparse transformers},
  author={Child, Rewon and Gray, Scott and Radford, Alec and Sutskever, Ilya},
  journal={arXiv preprint arXiv:1904.10509},
  year={2019}
}

@inproceedings{katharopoulos2020transformers,
  title={Transformers are rnns: Fast autoregressive transformers with linear attention},
  author={Katharopoulos, Angelos and Vyas, Apoorv and Pappas, Nikolaos and Fleuret, Fran{\c{c}}ois},
  booktitle={International conference on machine learning},
  pages={5156--5165},
  year={2020},
  organization={PMLR}
}


@article{abbe2022learning,
 author = {Abbe, Emmanuel and Bengio, Samy and Cornacchia, Elisabetta and Kleinberg, Jon and Lotfi, Aryo and Raghu, Maithra and Zhang, Chiyuan},
 journal = {Advances in Neural Information Processing Systems},
 pages = {2709--2722},
 title = {Learning to reason with neural networks: Generalization, unseen data and boolean measures},
 volume = {35},
 year = {2022}
}

@article{abbe2024far,
 author = {Abbe, Emmanuel and Bengio, Samy and Lotfi, Aryo and Sandon, Colin and Saremi, Omid},
 journal = {arXiv preprint arXiv:2406.06467},
 title = {How Far Can Transformers Reason? The Locality Barrier and Inductive Scratchpad},
 year = {2024}
}
@article{xu2025overview,
  title={An overview of condensation phenomenon in deep learning},
  author={Xu, Zhi-Qin John and Zhang, Yaoyu and Zhou, Zhangchen},
  journal={arXiv preprint arXiv:2504.09484},
  year={2025}
}

@article{xu2022overview,
  title={Overview frequency principle/spectral bias in deep learning},
  author={Xu, Zhi-Qin John and Zhang, Yaoyu and Luo, Tao},
  journal={Communications on Applied Mathematics and Computation},
  volume={7},
  number={3},
  pages={827--864},
  year={2025},
  publisher={Springer}
}
@article{abdin2024phi,
 author = {Abdin, Marah and Jacobs, Sam Ade and Awan, Ammar Ahmad and Aneja, Jyoti and Awadallah, Ahmed and Awadalla, Hany and Bach, Nguyen and Bahree, Amit and Bakhtiari, Arash and Behl, Harkirat and others},
 journal = {arXiv preprint arXiv:2404.14219},
 title = {Phi-3 technical report: A highly capable language model locally on your phone},
 year = {2024}
}
@article{gu2023mamba,
  title={Mamba: Linear-time sequence modeling with selective state spaces},
  author={Gu, Albert and Dao, Tri},
  journal={The Conference on Language Modeling},
  year={2024}
}
@article{dao2024transformers,
  title={Transformers are ssms: Generalized models and efficient algorithms through structured state space duality},
  author={Dao, Tri and Gu, Albert},
  journal={arXiv preprint arXiv:2405.21060},
  year={2024}
}

@inproceedings{akyrek2023what,
 author = {Ekin Akyürek and Dale Schuurmans and Jacob Andreas and Tengyu Ma and Denny Zhou},
 booktitle = {The Eleventh International Conference on Learning Representations },
 title = {What learning algorithm is in-context learning? Investigations with linear models},
 url = {https://openreview.net/forum?id=0g0X4H8yN4I},
 year = {2023}
}

@article{amsel2024benefits,
 author = {Amsel, Noah and Yehudai, Gilad and Bruna, Joan},
 journal = {arXiv preprint arXiv:2407.16153},
 title = {On the Benefits of Rank in Attention Layers},
 year = {2024}
}

@article{arora2018convergence,
 author = {Arora, Sanjeev and Cohen, Nadav and Golowich, Noah and Hu, Wei},
 journal = {arXiv preprint arXiv:1810.02281},
 title = {A convergence analysis of gradient descent for deep linear neural networks},
 year = {2018}
}

@inproceedings{arora2018optimization,
 author = {Arora, Sanjeev and Cohen, Nadav and Hazan, Elad},
 booktitle = {International Conference on Machine Learning},
 organization = {PMLR},
 pages = {244--253},
 title = {On the optimization of deep networks: Implicit acceleration by overparameterization},
 year = {2018}
}

@article{arora2019exact,
 author = {Arora, Sanjeev and Du, Simon S and Hu, Wei and Li, Zhiyuan and Salakhutdinov, Russ R and Wang, Ruosong},
 journal = {Advances in neural information processing systems},
 title = {On exact computation with an infinitely wide neural net},
 volume = {32},
 year = {2019}
}

@inproceedings{arora2019fine,
 author = {Arora, Sanjeev and Du, Simon and Hu, Wei and Li, Zhiyuan and Wang, Ruosong},
 booktitle = {International Conference on Machine Learning},
 organization = {PMLR},
 pages = {322--332},
 title = {Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks},
 year = {2019}
}

@article{arora2019implicit,
 author = {Arora, Sanjeev and Cohen, Nadav and Hu, Wei and Luo, Yuping},
 journal = {Advances in Neural Information Processing Systems},
 title = {Implicit regularization in deep matrix factorization},
 volume = {32},
 year = {2019}
}

@inproceedings{arora2022understanding,
 author = {Arora, Sanjeev and Li, Zhiyuan and Panigrahi, Abhishek},
 booktitle = {International Conference on Machine Learning},
 organization = {PMLR},
 pages = {948--1024},
 title = {Understanding gradient descent on the edge of stability in deep learning},
 year = {2022}
}

@article{aubry2024transformer,
 author = {Aubry, Murdock and Meng, Haoming and Sugolov, Anton and Papyan, Vardan},
 journal = {arXiv preprint arXiv:2407.07810},
 title = {Transformer Block Coupling and its Correlation with Generalization in LLMs},
 year = {2024}
}

@article{ba2016layer,
 author = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E},
 journal = {arXiv preprint arXiv:1607.06450},
 title = {Layer normalization},
 year = {2016}
}

@inproceedings{bai2023transformers,
 author = {Yu Bai and Fan Chen and Huan Wang and Caiming Xiong and Song Mei},
 booktitle = {Thirty-seventh Conference on Neural Information Processing Systems},
 title = {Transformers as Statisticians: Provable In-Context Learning with In-Context Algorithm Selection},
 url = {https://openreview.net/forum?id=liMSqUuVg9},
 year = {2023}
}

@inproceedings{bai2024connectivity,
 author = {Zhiwei Bai and Jiajie Zhao and Yaoyu Zhang},
 booktitle = {The Thirty-eighth Annual Conference on Neural Information Processing Systems},
 title = {Connectivity Shapes Implicit Regularization in Matrix Factorization Models for Matrix Completion},
 url = {https://openreview.net/forum?id=9jgODkdH0F},
 year = {2024}
}

@article{bartlett2002rademacher,
 author = {Bartlett, Peter L and Mendelson, Shahar},
 journal = {Journal of Machine Learning Research},
 number = {Nov},
 pages = {463--482},
 title = {Rademacher and Gaussian complexities: Risk bounds and structural results},
 volume = {3},
 year = {2002}
}

@article{bhuiya2024seemingly,
 author = {Bhuiya, Neeladri and Schlegel, Viktor and Winkler, Stefan},
 journal = {arXiv preprint arXiv:2409.05197},
 title = {Seemingly Plausible Distractors in Multi-Hop Reasoning: Are Large Language Models Attentive Readers?},
 year = {2024}
}

@inproceedings{bietti2023birth,
 author = {Alberto Bietti and Vivien Cabannes and Diane Bouchacourt and Herve Jegou and Leon Bottou},
 booktitle = {Thirty-seventh Conference on Neural Information Processing Systems},
 title = {Birth of a Transformer: A Memory Viewpoint},
 url = {https://openreview.net/forum?id=3X2EbBLNsk},
 year = {2023}
}

@article{bietti2024birth,
 author = {Bietti, Alberto and Cabannes, Vivien and Bouchacourt, Diane and Jegou, Herve and Bottou, Leon},
 journal = {Advances in Neural Information Processing Systems},
 title = {Birth of a transformer: A memory viewpoint},
 volume = {36},
 year = {2024}
}

@article{biran2024hopping,
 author = {Biran, Eden and Gottesman, Daniela and Yang, Sohee and Geva, Mor and Globerson, Amir},
 journal = {arXiv preprint arXiv:2406.12775},
 title = {Hopping Too Late: Exploring the Limitations of Large Language Models on Multi-Hop Queries},
 year = {2024}
}

@article{blanchard2011generalizing,
 author = {Blanchard, Gilles and Lee, Gyemin and Scott, Clayton},
 journal = {Advances in neural information processing systems},
 title = {Generalizing from several related classification tasks to a new unlabeled sample},
 volume = {24},
 year = {2011}
}

@article{boix2023can,
 author = {Boix-Adsera, Enric and Saremi, Omid and Abbe, Emmanuel and Bengio, Samy and Littwin, Etai and Susskind, Joshua},
 journal = {arXiv preprint arXiv:2310.09753},
 title = {When can transformers reason with abstract symbols?},
 year = {2023}
}

@article{brinkmann2024mechanistic,
 author = {Brinkmann, Jannik and Sheshadri, Abhay and Levoso, Victor and Swoboda, Paul and Bartelt, Christian},
 journal = {arXiv preprint arXiv:2402.11917},
 title = {A mechanistic analysis of a transformer trained on a symbolic multi-step reasoning task},
 year = {2024}
}

@article{brown2020language,
 author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
 journal = {Advances in neural information processing systems},
 pages = {1877--1901},
 title = {Language models are few-shot learners},
 volume = {33},
 year = {2020}
}

@misc{cabannes2024learningassociativememoriesgradient,
 archiveprefix = {arXiv},
 author = {Vivien Cabannes and Berfin Simsek and Alberto Bietti},
 eprint = {2402.18724},
 primaryclass = {cs.LG},
 title = {Learning Associative Memories with Gradient Descent},
 url = {https://arxiv.org/abs/2402.18724},
 year = {2024}
}

@article{chen2023phase,
 author = {Chen, Zhengan and Li, Yuqing and Luo, Tao and Zhou, Zhangchen and Xu, Zhi-Qin John},
 journal = {arXiv preprint arXiv:2303.06561},
 title = {Phase diagram of initial condensation for two-layer neural networks},
 year = {2023}
}

@article{chen2024can,
 author = {Chen, Xingwu and Zou, Difan},
 journal = {arXiv preprint arXiv:2404.01601},
 title = {What Can Transformer Learn with Varying Depth? Case Studies on Sequence Learning Tasks},
 year = {2024}
}

@article{chen2024training,
 author = {Chen, Siyu and Sheen, Heejune and Wang, Tianhao and Yang, Zhuoran},
 journal = {arXiv preprint arXiv:2402.19442},
 title = {Training dynamics of multi-head softmax attention for in-context learning: Emergence, convergence, and optimality},
 year = {2024}
}

@article{chen2024truncating,
 author = {Chen, Lei and Bruna, Joan and Bietti, Alberto},
 journal = {arXiv preprint arXiv:2406.03068},
 title = {How Truncating Weights Improves Reasoning in Language Models},
 year = {2024}
}

@article{chizat2018global,
 author = {Chizat, Lenaic and Bach, Francis},
 journal = {Advances in neural information processing systems},
 title = {On the global convergence of gradient descent for over-parameterized models using optimal transport},
 volume = {31},
 year = {2018}
}

@article{chizat2019lazy,
 author = {Chizat, Lenaic and Oyallon, Edouard and Bach, Francis},
 journal = {Advances in neural information processing systems},
 title = {On lazy training in differentiable programming},
 volume = {32},
 year = {2019}
}

@article{conmy2023towards,
 author = {Conmy, Arthur and Mavor-Parker, Augustine and Lynch, Aengus and Heimersheim, Stefan and Garriga-Alonso, Adrià},
 journal = {Advances in Neural Information Processing Systems},
 pages = {16318--16352},
 title = {Towards automated circuit discovery for mechanistic interpretability},
 volume = {36},
 year = {2023}
}

@article{dar2022analyzing,
 author = {Dar, Guy and Geva, Mor and Gupta, Ankit and Berant, Jonathan},
 journal = {arXiv preprint arXiv:2209.02535},
 title = {Analyzing transformers in embedding space},
 year = {2022}
}

@article{davies2021advancing,
 author = {Davies, Alex and Veličković, Petar and Buesing, Lars and Blackwell, Sam and Zheng, Daniel and Tomašev, Nenad and Tanburn, Richard and Battaglia, Peter and Blundell, Charles and Juhász, András and others},
 journal = {Nature},
 number = {7887},
 pages = {70--74},
 publisher = {Nature Publishing Group UK London},
 title = {Advancing mathematics by guiding human intuition with AI},
 volume = {600},
 year = {2021}
}

@article{devlin2018bert,
 author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
 journal = {arXiv preprint arXiv:1810.04805},
 title = {Bert: Pre-training of deep bidirectional transformers for language understanding},
 year = {2018}
}

@article{dong2022survey,
 author = {Dong, Qingxiu and Li, Lei and Dai, Damai and Zheng, Ce and Wu, Zhiyong and Chang, Baobao and Sun, Xu and Xu, Jingjing and Sui, Zhifang},
 journal = {arXiv preprint arXiv:2301.00234},
 title = {A survey on in-context learning},
 year = {2022}
}

@article{dutta2024think,
 author = {Dutta, Subhabrata and Singh, Joykirat and Chakrabarti, Soumen and Chakraborty, Tanmoy},
 journal = {arXiv preprint arXiv:2402.18312},
 title = {How to think step-by-step: A mechanistic understanding of chain-of-thought reasoning},
 year = {2024}
}

@article{e2020comparative,
 author = {E, Weinan and Ma, Chao and Wu, Lei },
 journal = {Sci. China Math.},
 title = {A comparative analysis of optimization and generalization properties of two-layer neural network and random feature models under gradient descent dynamics.},
 volume = {63},
 year = {2020}
}

@article{edelman2024evolution,
 author = {Edelman, Benjamin L and Edelman, Ezra and Goel, Surbhi and Malach, Eran and Tsilivis, Nikolaos},
 journal = {arXiv preprint arXiv:2402.11004},
 title = {The Evolution of Statistical Induction Heads: In-Context Learning Markov Chains},
 year = {2024}
}

@misc{edelman2024evolutionstatisticalinductionheads,
 archiveprefix = {arXiv},
 author = {Benjamin L. Edelman and Ezra Edelman and Surbhi Goel and Eran Malach and Nikolaos Tsilivis},
 eprint = {2402.11004},
 primaryclass = {cs.LG},
 title = {The Evolution of Statistical Induction Heads: In-Context Learning Markov Chains},
 url = {https://arxiv.org/abs/2402.11004},
 year = {2024}
}

@article{elhage2021mathematical,
 author = {Elhage, Nelson and Nanda, Neel and Olsson, Catherine and Henighan, Tom and Joseph, Nicholas and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and DasSarma, Nova and Drain, Dawn and Ganguli, Deep and Hatfield-Dodds, Zac and Hernandez, Danny and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
 journal = {Transformer Circuits Thread},
 note = {https://transformer-circuits.pub/2021/framework/index.html},
 title = {A Mathematical Framework for Transformer Circuits},
 year = {2021}
}

@article{feng2023language,
 author = {Feng, Jiahai and Steinhardt, Jacob},
 journal = {arXiv preprint arXiv:2310.17191},
 title = {How do language models bind entities in context?},
 year = {2023}
}

@article{feng2023towards,
 author = {Feng, Guhao and Gu, Yuntian and Zhang, Bohang and Ye, Haotian and He, Di and Wang, Liwei},
 journal = {Thirty-seventh Conference on Neural Information Processing Systems (Spotlight)},
 title = {Towards Revealing the Mystery behind Chain of Thought: a Theoretical Perspective},
 volume = {36},
 year = {2023}
}

@article{garg2022can,
 author = {Garg, Shivam and Tsipras, Dimitris and Liang, Percy S and Valiant, Gregory},
 journal = {Advances in Neural Information Processing Systems},
 pages = {30583--30598},
 title = {What can transformers learn in-context? a case study of simple function classes},
 volume = {35},
 year = {2022}
}

@article{goldowsky2023localizing,
 author = {Goldowsky-Dill, Nicholas and MacLeod, Chris and Sato, Lucas and Arora, Aryaman},
 journal = {arXiv preprint arXiv:2304.05969},
 title = {Localizing model behavior with path patching},
 year = {2023}
}

@inproceedings{gu2022efficiently,
 author = {Albert Gu and Karan Goel and Christopher Re},
 booktitle = {International Conference on Learning Representations},
 title = {Efficiently Modeling Long Sequences with Structured State Spaces},
 url = {https://openreview.net/forum?id=uYLFoz1vlAC},
 year = {2022}
}

@article{gunasekar2017implicit,
 author = {Gunasekar, Suriya and Woodworth, Blake E and Bhojanapalli, Srinadh and Neyshabur, Behnam and Srebro, Nati},
 journal = {Advances in neural information processing systems},
 title = {Implicit regularization in matrix factorization},
 volume = {30},
 year = {2017}
}

@article{guo2023transformers,
 author = {Guo, Tianyu and Hu, Wei and Mei, Song and Wang, Huan and Xiong, Caiming and Savarese, Silvio and Bai, Yu},
 journal = {arXiv preprint arXiv:2310.10616},
 title = {How do transformers learn in-context beyond simple functions? a case study on learning with representations},
 year = {2023}
}

@inproceedings{guo2024how,
 author = {Tianyu Guo and Wei Hu and Song Mei and Huan Wang and Caiming Xiong and Silvio Savarese and Yu Bai},
 booktitle = {The Twelfth International Conference on Learning Representations},
 title = {How Do Transformers Learn In-Context Beyond Simple Functions? A Case Study on Learning with Representations},
 url = {https://openreview.net/forum?id=ikwEDva1JZ},
 year = {2024}
}

@article{hou2023towards,
 author = {Hou, Yifan and Li, Jiaoda and Fei, Yu and Stolfo, Alessandro and Zhou, Wangchunshu and Zeng, Guangtao and Bosselut, Antoine and Sachan, Mrinmaya},
 journal = {arXiv preprint arXiv:2310.14491},
 title = {Towards a mechanistic interpretation of multi-step reasoning capabilities of language models},
 year = {2023}
}

@inproceedings{huang2020dynamics,
 author = {Huang, Jiaoyang and Yau, Horng-Tzer},
 booktitle = {International conference on machine learning},
 organization = {PMLR},
 pages = {4542--4551},
 title = {Dynamics of deep neural networks and neural tangent hierarchy},
 year = {2020}
}

@inproceedings{huang2020improving,
 author = {Huang, Xiao Shi and Perez, Felipe and Ba, Jimmy and Volkovs, Maksims},
 booktitle = {International Conference on Machine Learning},
 organization = {PMLR},
 pages = {4475--4483},
 title = {Improving transformer optimization through better initialization},
 year = {2020}
}

@misc{huang2023incontextconvergencetransformers,
 archiveprefix = {arXiv},
 author = {Yu Huang and Yuan Cheng and Yingbin Liang},
 eprint = {2310.05249},
 primaryclass = {cs.LG},
 title = {In-Context Convergence of Transformers},
 url = {https://arxiv.org/abs/2310.05249},
 year = {2023}
}

@article{ildiz2024self,
 author = {Ildiz, M Emrullah and Huang, Yixiao and Li, Yingcong and Rawat, Ankit Singh and Oymak, Samet},
 journal = {arXiv preprint arXiv:2402.13512},
 title = {From self-attention to markov models: Unveiling the dynamics of generative transformers},
 year = {2024}
}

@article{jacot2018neural,
 author = {Jacot, Arthur and Gabriel, Franck and Hongler, Clément},
 journal = {Advances in neural information processing systems},
 title = {Neural tangent kernel: Convergence and generalization in neural networks},
 volume = {31},
 year = {2018}
}

@inproceedings{jacot2020implicit,
 author = {Jacot, Arthur and Simsek, Berfin and Spadaro, Francesco and Hongler, Clément and Gabriel, Franck},
 booktitle = {International Conference on Machine Learning},
 organization = {PMLR},
 pages = {4631--4640},
 title = {Implicit regularization of random feature models},
 year = {2020}
}

@article{jeoung2022changed,
 author = {Jeoung, Sullam and Diesner, Jana},
 journal = {arXiv preprint arXiv:2206.00701},
 title = {What changed? investigating debiasing methods using causal mediation analysis},
 year = {2022}
}

@article{ji2018gradient,
 author = {Ji, Ziwei and Telgarsky, Matus},
 journal = {arXiv preprint arXiv:1810.02032},
 title = {Gradient descent aligns the layers of deep linear networks},
 year = {2018}
}

@inproceedings{ji2019gradient,
 author = {Ji, Ziwei and Telgarsky, Matus},
 booktitle = {7th International Conference on Learning Representations, ICLR 2019},
 title = {Gradient descent aligns the layers of deep linear networks},
 year = {2019}
}

@article{jiang2024peek,
 author = {Jiang, Bowen and Xie, Yangxinyu and Hao, Zhuoqun and Wang, Xiaomeng and Mallick, Tanwi and Su, Weijie J and Taylor, Camillo J and Roth, Dan},
 journal = {arXiv preprint arXiv:2406.11050},
 title = {A Peek into Token Bias: Large Language Models Are Not Yet Genuine Reasoners},
 year = {2024}
}

@article{kil2024ii,
 author = {Kil, Jihyung and Tavazoee, Farideh and Kang, Dongyeop and Kim, Joo-Kyung},
 journal = {arXiv preprint arXiv:2402.11058},
 title = {II-MMR: Identifying and improving multi-modal multi-hop reasoning in visual question answering},
 year = {2024}
}

@article{kobayashi2020attention,
 author = {Kobayashi, Goro and Kuribayashi, Tatsuki and Yokoi, Sho and Inui, Kentaro},
 journal = {arXiv preprint arXiv:2004.10102},
 title = {Attention is not only a weight: Analyzing transformers with vector norms},
 year = {2020}
}

@article{kojima2022large,
 author = {Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
 journal = {Advances in neural information processing systems},
 pages = {22199--22213},
 title = {Large language models are zero-shot reasoners},
 volume = {35},
 year = {2022}
}

@article{kovaleva2019revealing,
 author = {Kovaleva, Olga and Romanov, Alexey and Rogers, Anna and Rumshisky, Anna},
 journal = {arXiv preprint arXiv:1908.08593},
 title = {Revealing the dark secrets of BERT},
 year = {2019}
}

@article{kumar2024early,
 author = {Kumar, Akshay and Haupt, Jarvis},
 journal = {arXiv preprint arXiv:2403.08121},
 title = {Early Directional Convergence in Deep Homogeneous Neural Networks for Small Initializations},
 year = {2024}
}

@inproceedings{li2018algorithmic,
 author = {Li, Yuanzhi and Ma, Tengyu and Zhang, Hongyang},
 booktitle = {Conference On Learning Theory},
 organization = {PMLR},
 pages = {2--47},
 title = {Algorithmic regularization in over-parameterized matrix sensing and neural networks with quadratic activations},
 year = {2018}
}

@article{li2021validity,
 author = {Li, Zhiyuan and Malladi, Sadhika and Arora, Sanjeev},
 journal = {Advances in Neural Information Processing Systems},
 pages = {12712--12725},
 title = {On the validity of modeling sgd with stochastic differential equations (sdes)},
 volume = {34},
 year = {2021}
}

@misc{li2023transformerslearntopicstructure,
 archiveprefix = {arXiv},
 author = {Yuchen Li and Yuanzhi Li and Andrej Risteski},
 eprint = {2303.04245},
 primaryclass = {cs.LG},
 title = {How Do Transformers Learn Topic Structure: Towards a Mechanistic Understanding},
 url = {https://arxiv.org/abs/2303.04245},
 year = {2023}
}

@article{li2024chain,
 author = {Li, Zhiyuan and Liu, Hong and Zhou, Denny and Ma, Tengyu},
 journal = {arXiv preprint arXiv:2402.12875},
 title = {Chain of thought empowers transformers to solve inherently serial problems},
 year = {2024}
}

@article{li2024making,
 author = {Li, Yanyang and Liang, Shuo and Lyu, Michael R and Wang, Liwei},
 journal = {arXiv preprint arXiv:2408.03246},
 title = {Making long-context language models better multi-hop reasoners},
 year = {2024}
}

@article{li2024understanding,
 author = {Li, Zhaoyi and Jiang, Gangwei and Xie, Hong and Song, Linqi and Lian, Defu and Wei, Ying},
 journal = {arXiv preprint arXiv:2402.14328},
 title = {Understanding and patching compositional reasoning in llms},
 year = {2024}
}

@article{liu2018generating,
 author = {Liu, Peter J and Saleh, Mohammad and Pot, Etienne and Goodrich, Ben and Sepassi, Ryan and Kaiser, Lukasz and Shazeer, Noam},
 journal = {arXiv preprint arXiv:1801.10198},
 title = {Generating wikipedia by summarizing long sequences},
 year = {2018}
}

@article{liu2020understanding,
 author = {Liu, Liyuan and Liu, Xiaodong and Gao, Jianfeng and Chen, Weizhu and Han, Jiawei},
 journal = {arXiv preprint arXiv:2004.08249},
 title = {Understanding the difficulty of training transformers},
 year = {2020}
}

@inproceedings{liu2022omnigrok,
 author = {Liu, Ziming and Michaud, Eric J and Tegmark, Max},
 booktitle = {The Eleventh International Conference on Learning Representations},
 title = {Omnigrok: Grokking beyond algorithmic data},
 year = {2022}
}

@article{liu2023neuron,
 author = {Liu, Yibing and Tian, Chris Xing and Li, Haoliang and Ma, Lei and Wang, Shiqi},
 journal = {arXiv preprint arXiv:2306.02879},
 title = {Neuron activation coverage: Rethinking out-of-distribution detection and generalization},
 year = {2023}
}

@inproceedings{lu2021on,
 author = {Haoye Lu and Yongyi Mao and Amiya Nayak},
 booktitle = {International Conference on Learning Representations},
 title = {On the Dynamics of Training Attention Models},
 url = {https://openreview.net/forum?id=1OCTOShAmqB},
 year = {2021}
}

@article{luo2021phase,
 author = {Luo, Tao and Xu, Zhi-Qin John and Ma, Zheng and Zhang, Yaoyu},
 journal = {The Journal of Machine Learning Research},
 number = {1},
 pages = {3327--3373},
 publisher = {JMLRORG},
 title = {Phase diagram for two-layer relu neural networks at infinite-width limit},
 volume = {22},
 year = {2021}
}

@article{maennel2018gradient,
 author = {Maennel, Hartmut and Bousquet, Olivier and Gelly, Sylvain},
 journal = {arXiv preprint arXiv:1803.08367},
 title = {Gradient descent quantizes relu network features},
 year = {2018}
}

@inproceedings{mahankali2024one,
 author = {Arvind V. Mahankali and Tatsunori Hashimoto and Tengyu Ma},
 booktitle = {The Twelfth International Conference on Learning Representations},
 title = {One Step of Gradient Descent is Provably the Optimal In-Context Learner with One Layer of Linear Self-Attention},
 url = {https://openreview.net/forum?id=8p3fu56lKc},
 year = {2024}
}

@article{mei2018mean,
 author = {Mei, Song and Montanari, Andrea and Nguyen, Phan-Minh},
 journal = {Proceedings of the National Academy of Sciences},
 number = {33},
 pages = {E7665--E7671},
 publisher = {National Acad Sciences},
 title = {A mean field view of the landscape of two-layer neural networks},
 volume = {115},
 year = {2018}
}

@article{meng2022locating,
 author = {Meng, Kevin and Bau, David and Andonian, Alex and Belinkov, Yonatan},
 journal = {Advances in Neural Information Processing Systems},
 pages = {17359--17372},
 title = {Locating and editing factual associations in GPT},
 volume = {35},
 year = {2022}
}

@article{merullo2023circuit,
 author = {Merullo, Jack and Eickhoff, Carsten and Pavlick, Ellie},
 journal = {arXiv preprint arXiv:2310.08744},
 title = {Circuit component reuse across tasks in transformer language models},
 year = {2023}
}

@article{min2022rethinking,
 author = {Min, Sewon and Lyu, Xinxi and Holtzman, Ari and Artetxe, Mikel and Lewis, Mike and Hajishirzi, Hannaneh and Zettlemoyer, Luke},
 journal = {arXiv preprint arXiv:2202.12837},
 title = {Rethinking the role of demonstrations: What makes in-context learning work?},
 year = {2022}
}

@inproceedings{min21c,
 abstract = {Neural networks trained via gradient descent with random initialization and without any regularization enjoy good generalization performance in practice despite being highly overparametrized. A promising direction to explain this phenomenon is to study how initialization and overparametrization affect convergence and implicit bias of training algorithms. In this paper, we present a novel analysis of single-hidden-layer linear networks trained under gradient flow, which connects initialization, optimization, and overparametrization. Firstly, we show that the squared loss converges exponentially to its optimum at a rate that depends on the level of imbalance of the initialization. Secondly, we show that proper initialization constrains the dynamics of the network parameters to lie within an invariant set. In turn, minimizing the loss over this set leads to the min-norm solution. Finally, we show that large hidden layer width, together with (properly scaled) random initialization, ensures proximity to such an invariant set during training, allowing us to derive a novel non-asymptotic upper-bound on the distance between the trained network and the min-norm solution.},
 author = {Min, Hancheng and Tarmoun, Salma and Vidal, Rene and Mallada, Enrique},
 booktitle = {Proceedings of the 38th International Conference on Machine Learning},
 editor = {Meila, Marina and Zhang, Tong},
 month = {18--24 Jul},
 pages = {7760--7768},
 pdf = {http://proceedings.mlr.press/v139/min21c/min21c.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {On the Explicit Role of Initialization on the Convergence and Implicit Bias of Overparametrized Linear Networks},
 url = {https://proceedings.mlr.press/v139/min21c.html},
 volume = {139},
 year = {2021}
}

@article{muller2021transformers,
 author = {Müller, Samuel and Hollmann, Noah and Arango, Sebastian Pineda and Grabocka, Josif and Hutter, Frank},
 journal = {arXiv preprint arXiv:2112.10510},
 title = {Transformers can do bayesian inference},
 year = {2021}
}

@inproceedings{NEURIPS2024_a8633d27,
 author = {Collins, Liam and Parulekar, Advait and Mokhtari, Aryan and Sanghavi, Sujay and Shakkottai, Sanjay},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {92638--92696},
 publisher = {Curran Associates, Inc.},
 title = {In-Context Learning with Transformers: Softmax Attention Adapts to Function Lipschitzness},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/a8633d27d782f66fe660c2fb4bae446e-Paper-Conference.pdf},
 volume = {37},
 year = {2024}
}

@inproceedings{nichani2024how,
 author = {Eshaan Nichani and Alex Damian and Jason D. Lee},
 booktitle = {Forty-first International Conference on Machine Learning},
 title = {How Transformers Learn Causal Structure with Gradient Descent},
 url = {https://openreview.net/forum?id=jNM4imlHZv},
 year = {2024}
}

@article{nichani2024transformers,
 author = {Nichani, Eshaan and Damian, Alex and Lee, Jason D},
 journal = {arXiv preprint arXiv:2402.14735},
 title = {How Transformers Learn Causal Structure with Gradient Descent},
 year = {2024}
}

@article{olsson2022context,
 author = {Olsson, Catherine and Elhage, Nelson and Nanda, Neel and Joseph, Nicholas and DasSarma, Nova and Henighan, Tom and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and others},
 journal = {arXiv preprint arXiv:2209.11895},
 title = {In-context learning and induction heads},
 year = {2022}
}

@misc{olsson2022incontextlearninginductionheads,
 archiveprefix = {arXiv},
 author = {Catherine Olsson and Nelson Elhage and Neel Nanda and Nicholas Joseph and Nova DasSarma and Tom Henighan and Ben Mann and Amanda Askell and Yuntao Bai and Anna Chen and Tom Conerly and Dawn Drain and Deep Ganguli and Zac Hatfield-Dodds and Danny Hernandez and Scott Johnston and Andy Jones and Jackson Kernion and Liane Lovitt and Kamal Ndousse and Dario Amodei and Tom Brown and Jack Clark and Jared Kaplan and Sam McCandlish and Chris Olah},
 eprint = {2209.11895},
 primaryclass = {cs.LG},
 title = {In-context Learning and Induction Heads},
 url = {https://arxiv.org/abs/2209.11895},
 year = {2022}
}

@misc{openai2023gpt4,
 archiveprefix = {arXiv},
 author = {OpenAI},
 eprint = {2303.08774},
 primaryclass = {cs.CL},
 title = {GPT-4 Technical Report},
 year = {2023}
}

@article{phasediagram,
 abstract = {<p style="text-align: justify;">The phenomenon of distinct behaviors exhibited by neural networks under
varying scales of initialization remains an enigma in deep learning research. In this
paper, based on the earlier work [Luo et al., J. Mach. Learn. Res., 22:1–47, 2021], we
present a phase diagram of initial condensation for two-layer neural networks. Condensation is a phenomenon wherein the weight vectors of neural networks concentrate
on isolated orientations during the training process, and it is a feature in non-linear
learning process that enables neural networks to possess better generalization abilities.
Our phase diagram serves to provide a comprehensive understanding of the dynamical regimes of neural networks and their dependence on the choice of hyperparameters
related to initialization. Furthermore, we demonstrate in detail the underlying mechanisms by which small initialization leads to condensation at the initial training stage.</p>},
 author = {Zheng-An, Chen and Yuqing, Li and Tao, Luo and Zhou, Zhangchen and Zhi-Qin, Xu, John},
 doi = {https://doi.org/10.4208/csiam-am.SO-2023-0016},
 issn = {2708-0579},
 journal = {CSIAM Transactions on Applied Mathematics},
 number = {3},
 pages = {448--514},
 title = {Phase Diagram of Initial Condensation for Two-Layer Neural Networks},
 url = {https://global-sci.com/article/91025/phase-diagram-of-initial-condensation-for-two-layer-neural-networks},
 volume = {5},
 year = {2024}
}

@inproceedings{pmlr-v161-bachlechner21a,
 abstract = {Deep networks often suffer from vanishing or exploding gradients due to inefficient signal propagation, leading to long training times or convergence difficulties. Various architecture designs, sophisticated residual-style networks, and initialization schemes have been shown to improve deep signal propagation. Recently, Pennington et al. [2017] used free probability theory to show that dynamical isometry plays an integral role in efficient deep learning. We show that the simplest architecture change of gating each residual connection using a single zero-initialized parameter satisfies initial dynamical isometry and outperforms more complex approaches. Although much simpler than its predecessors, this gate enables training thousands of fully connected layers with fast convergence and better test performance for ResNets trained on an image recognition task. We apply this technique to language modeling and find that we can easily train 120-layer Transformers. When applied to 12 layer Transformers, it converges 56% faster.},
 author = {Bachlechner, Thomas and Majumder, Bodhisattwa Prasad and Mao, Henry and Cottrell, Gary and McAuley, Julian},
 booktitle = {Proceedings of the Thirty-Seventh Conference on Uncertainty in Artificial Intelligence},
 editor = {de Campos, Cassio and Maathuis, Marloes H.},
 month = {27--30 Jul},
 pages = {1352--1361},
 pdf = {https://proceedings.mlr.press/v161/bachlechner21a/bachlechner21a.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {ReZero is all you need: fast convergence at large depth},
 url = {https://proceedings.mlr.press/v161/bachlechner21a.html},
 volume = {161},
 year = {2021}
}

@article{poli2024mechanistic,
 author = {Poli, Michael and Thomas, Armin W and Nguyen, Eric and Ponnusamy, Pragaash and Deiseroth, Björn and Kersting, Kristian and Suzuki, Taiji and Hie, Brian and Ermon, Stefano and Ré, Christopher and others},
 journal = {arXiv preprint arXiv:2403.17844},
 title = {Mechanistic Design and Scaling of Hybrid Architectures},
 year = {2024}
}

@article{polu2020generative,
 author = {Polu, Stanislas and Sutskever, Ilya},
 journal = {arXiv preprint arXiv:2009.03393},
 title = {Generative language modeling for automated theorem proving},
 year = {2020}
}

@article{power2022grokking,
 author = {Power, Alethea and Burda, Yuri and Edwards, Harri and Babuschkin, Igor and Misra, Vedant},
 journal = {arXiv preprint arXiv:2201.02177},
 title = {Grokking: Generalization beyond overfitting on small algorithmic datasets},
 year = {2022}
}

@article{radford2019language,
 author = {Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
 title = {Language Models are Unsupervised Multitask Learners},
 year = {2019}
}

@article{reddy2023mechanistic,
 author = {Reddy, Gautam},
 journal = {arXiv preprint arXiv:2312.03002},
 title = {The mechanistic basis of data dependence and abrupt learning in an in-context classification task},
 year = {2023}
}

@inproceedings{reddy2024the,
 author = {Gautam Reddy},
 booktitle = {The Twelfth International Conference on Learning Representations},
 title = {The mechanistic basis of data dependence and abrupt learning in an in-context classification task},
 url = {https://openreview.net/forum?id=aN4Jf6Cx69},
 year = {2024}
}

@inproceedings{ren2024understanding,
 author = {Ren, Yinuo and Ma, Chao and Ying, Lexing},
 booktitle = {International Conference on Artificial Intelligence and Statistics},
 organization = {PMLR},
 pages = {4465--4473},
 title = {Understanding the Generalization Benefits of Late Learning Rate Decay},
 year = {2024}
}

@article{rotskoff2018parameters,
 author = {Rotskoff, Grant and Vanden-Eijnden, Eric},
 journal = {Advances in neural information processing systems},
 title = {Parameters as interacting particles: long time convergence and asymptotic error scaling of neural networks},
 volume = {31},
 year = {2018}
}

@article{sakarvadia2023memory,
 author = {Sakarvadia, Mansi and Ajith, Aswathy and Khan, Arham and Grzenda, Daniel and Hudson, Nathaniel and Bauer, André and Chard, Kyle and Foster, Ian},
 journal = {arXiv preprint arXiv:2309.05605},
 title = {Memory injections: Correcting multi-hop reasoning failures during inference in transformer-based language models},
 year = {2023}
}

@article{saparov2022language,
 author = {Saparov, Abulhair and He, He},
 journal = {arXiv preprint arXiv:2210.01240},
 title = {Language models are greedy reasoners: A systematic formal analysis of chain-of-thought},
 year = {2022}
}

@article{saxe2013exact,
 author = {Saxe, Andrew M and McClelland, James L and Ganguli, Surya},
 journal = {arXiv preprint arXiv:1312.6120},
 title = {Exact solutions to the nonlinear dynamics of learning in deep linear neural networks},
 year = {2013}
}

@article{shalev2024distributional,
 author = {Shalev, Yuval and Feder, Amir and Goldstein, Ariel},
 journal = {arXiv preprint arXiv:2406.13858},
 title = {Distributional reasoning in LLMs: Parallel reasoning processes in multi-hop reasoning},
 year = {2024}
}

@article{sharma2023truth,
 author = {Sharma, Pratyusha and Ash, Jordan T and Misra, Dipendra},
 journal = {arXiv preprint arXiv:2312.13558},
 title = {The truth is in there: Improving reasoning in language models with layer-selective rank reduction},
 year = {2023}
}

@article{sirignano2020mean,
 author = {Sirignano, Justin and Spiliopoulos, Konstantinos},
 journal = {Stochastic Processes and their Applications},
 number = {3},
 pages = {1820--1852},
 publisher = {Elsevier},
 title = {Mean field analysis of neural networks: A central limit theorem},
 volume = {130},
 year = {2020}
}

@article{Snell2021ApproximatingHS,
 author = {Charles Burton Snell and Ruiqi Zhong and Dan Klein and Jacob Steinhardt},
 journal = {ArXiv},
 title = {Approximating How Single Head Attention Learns},
 url = {https://api.semanticscholar.org/CorpusID:232232786},
 volume = {abs/2103.07601},
 year = {2021}
}

@inproceedings{soltanolkotabi2023implicit,
 author = {Soltanolkotabi, Mahdi and Stöger, Dominik and Xie, Changzhi},
 booktitle = {The Thirty Sixth Annual Conference on Learning Theory},
 organization = {PMLR},
 pages = {5140--5142},
 title = {Implicit balancing and regularization: Generalization and convergence guarantees for overparameterized asymmetric matrix sensing},
 year = {2023}
}

@article{sprague2023musr,
 author = {Sprague, Zayne and Ye, Xi and Bostrom, Kaj and Chaudhuri, Swarat and Durrett, Greg},
 journal = {arXiv preprint arXiv:2310.16049},
 title = {Musr: Testing the limits of chain-of-thought with multistep soft reasoning},
 year = {2023}
}

@article{stoger2021small,
 author = {Stöger, Dominik and Soltanolkotabi, Mahdi},
 journal = {Advances in Neural Information Processing Systems},
 pages = {23831--23843},
 title = {Small random initialization is akin to spectral learning: Optimization and generalization guarantees for overparameterized low-rank matrix reconstruction},
 volume = {34},
 year = {2021}
}

@article{tian2023scan,
 author = {Tian, Yuandong and Wang, Yiping and Chen, Beidi and Du, Simon S},
 journal = {Advances in neural information processing systems},
 pages = {71911--71947},
 title = {Scan and snap: Understanding training dynamics and token composition in 1-layer transformer},
 volume = {36},
 year = {2023}
}

@inproceedings{tian2024joma,
 author = {Yuandong Tian and Yiping Wang and Zhenyu Zhang and Beidi Chen and Simon Shaolei Du},
 booktitle = {The Twelfth International Conference on Learning Representations},
 title = {JoMA: Demystifying Multilayer Transformers via Joint Dynamics of MLP and Attention},
 url = {https://openreview.net/forum?id=LbJqRGNYCf},
 year = {2024}
}

@article{todd2023function,
 author = {Todd, Eric and Li, Millicent L and Sharma, Arnab Sen and Mueller, Aaron and Wallace, Byron C and Bau, David},
 journal = {arXiv preprint arXiv:2310.15213},
 title = {Function vectors in large language models},
 year = {2023}
}

@article{touvron2023llama,
 author = {Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timothée and Rozière, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
 journal = {arXiv preprint arXiv:2302.13971},
 title = {Llama: Open and efficient foundation language models},
 year = {2023}
}

@article{trinh2024solving,
 author = {Trinh, Trieu H and Wu, Yuhuai and Le, Quoc V and He, He and Luong, Thang},
 journal = {Nature},
 number = {7995},
 pages = {476--482},
 publisher = {Nature Publishing Group},
 title = {Solving olympiad geometry without human demonstrations},
 volume = {625},
 year = {2024}
}

@inproceedings{trockman2023mimetic,
 author = {Trockman, Asher and Kolter, J Zico},
 booktitle = {International Conference on Machine Learning},
 organization = {PMLR},
 pages = {34456--34468},
 title = {Mimetic initialization of self-attention layers},
 year = {2023}
}

@inproceedings{varre2023on,
 author = {Aditya Vardhan Varre and Maria-Luiza Vladarean and Loucas Pillaud-Vivien and Nicolas Flammarion},
 booktitle = {Thirty-seventh Conference on Neural Information Processing Systems},
 title = {On the spectral bias of two-layer linear networks},
 url = {https://openreview.net/forum?id=FFdrXkm3Cz},
 year = {2023}
}

@article{vaswani2017attention,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Lukasz and Polosukhin, Illia},
 journal = {Advances in neural information processing systems},
 title = {Attention is all you need},
 volume = {30},
 year = {2017}
}

@article{vig2019multiscale,
 author = {Vig, Jesse},
 journal = {arXiv preprint arXiv:1906.05714},
 title = {A multiscale visualization of attention in the transformer model},
 year = {2019}
}

@article{vig2020investigating,
 author = {Vig, Jesse and Gehrmann, Sebastian and Belinkov, Yonatan and Qian, Sharon and Nevo, Daniel and Singer, Yaron and Shieber, Stuart},
 journal = {Advances in neural information processing systems},
 pages = {12388--12401},
 title = {Investigating gender bias in language models using causal mediation analysis},
 volume = {33},
 year = {2020}
}
# mamba related works
@article{voelker2019dynamical,
 author = {Voelker, Aaron Russell},
 publisher = {University of Waterloo},
 title = {Dynamical systems in spiking neuromorphic hardware},
 year = {2019}
}

@article{voelker2019legendre,
 author = {Voelker, Aaron and Kajić, Ivana and Eliasmith, Chris},
 journal = {Advances in neural information processing systems},
 title = {Legendre memory units: Continuous-time representation in recurrent neural networks},
 volume = {32},
 year = {2019}
}

@article{voita2019analyzing,
 author = {Voita, Elena and Talbot, David and Moiseev, Fedor and Sennrich, Rico and Titov, Ivan},
 journal = {arXiv preprint arXiv:1905.09418},
 title = {Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned},
 year = {2019}
}

@inproceedings{
smith2023simplified,
title={Simplified State Space Layers for Sequence Modeling},
author={Jimmy T.H. Smith and Andrew Warrington and Scott Linderman},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=Ai8Hw3AXqks}
}
@inproceedings{rwkv45,
    title = "{RWKV}: Reinventing {RNN}s for the Transformer Era",
    author = "Peng, Bo  and
      Alcaide, Eric  and
      Anthony, Quentin  and
      Albalak, Alon  and
      Arcadinho, Samuel  and
      Biderman, Stella  and
      Cao, Huanqi  and
      Cheng, Xin  and
      Chung, Michael  and
      Derczynski, Leon  and
      Du, Xingjian  and
      Grella, Matteo  and
      Gv, Kranthi  and
      He, Xuzheng  and
      Hou, Haowen  and
      Kazienko, Przemyslaw  and
      Kocon, Jan  and
      Kong, Jiaming  and
      Koptyra, Bart{\l}omiej  and
      Lau, Hayden  and
      Lin, Jiaju  and
      Mantri, Krishna Sri Ipsit  and
      Mom, Ferdinand  and
      Saito, Atsushi  and
      Song, Guangyu  and
      Tang, Xiangru  and
      Wind, Johan  and
      Wo{\'z}niak, Stanis{\l}aw  and
      Zhang, Zhenyuan  and
      Zhou, Qinghua  and
      Zhu, Jian  and
      Zhu, Rui-Jie",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.936/",
    doi = "10.18653/v1/2023.findings-emnlp.936",
    pages = "14048--14077",
    abstract = "Transformers have revolutionized almost all natural language processing (NLP) tasks but suffer from memory and computational complexity that scales quadratically with sequence length. In contrast, recurrent neural networks (RNNs) exhibit linear scaling in memory and computational requirements but struggle to match the same performance as Transformers due to limitations in parallelization and scalability. We propose a novel model architecture, Receptance Weighted Key Value (RWKV), that combines the efficient parallelizable training of transformers with the efficient inference of RNNs. Our approach leverages a linear attention mechanism and allows us to formulate the model as either a Transformer or an RNN, thus parallelizing computations during training and maintains constant computational and memory complexity during inference. We scale our models as large as 14 billion parameters, by far the largest dense RNN ever trained, and find RWKV performs on par with similarly sized Transformers, suggesting future work can leverage this architecture to create more efficient models. This work presents a significant step towards reconciling trade-offs between computational efficiency and model performance in sequence processing tasks."
}
@inproceedings{
RWKV6,
title={Eagle and Finch: {RWKV} with Matrix-Valued States and Dynamic Recurrence},
author={Bo Peng and Daniel Goldstein and Quentin Gregory Anthony and Alon Albalak and Eric Alcaide and Stella Biderman and Eugene Cheah and Teddy Ferdinan and Kranthi Kiran GV and Haowen Hou and Satyapriya Krishna and Ronald McClelland Jr. and Niklas Muennighoff and Fares Obeid and Atsushi Saito and Guangyu Song and Haoqin Tu and Ruichong Zhang and Bingchen Zhao and Qihang Zhao and Jian Zhu and Rui-Jie Zhu},
booktitle={First Conference on Language Modeling},
year={2024},
url={https://openreview.net/forum?id=soz1SEiPeq}
}
@misc{sun2023retentivenetworksuccessortransformer,
      title={Retentive Network: A Successor to Transformer for Large Language Models}, 
      author={Yutao Sun and Li Dong and Shaohan Huang and Shuming Ma and Yuqing Xia and Jilong Xue and Jianyong Wang and Furu Wei},
      year={2023},
      eprint={2307.08621},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2307.08621}, 
}

@InProceedings{pmlr-v235-yang24ab,
  title = 	 {Gated Linear Attention Transformers with Hardware-Efficient Training},
  author =       {Yang, Songlin and Wang, Bailin and Shen, Yikang and Panda, Rameswar and Kim, Yoon},
  booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning},
  pages = 	 {56501--56523},
  year = 	 {2024},
  editor = 	 {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  volume = 	 {235},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {21--27 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://raw.githubusercontent.com/mlresearch/v235/main/assets/yang24ab/yang24ab.pdf},
  url = 	 {https://proceedings.mlr.press/v235/yang24ab.html},
  abstract = 	 {Transformers with linear attention allow for efficient parallel training but can simultaneously be formulated as an RNN with 2D (matrix-valued) hidden states, thus enjoying linear-time inference complexity. However, linear attention generally underperforms ordinary softmax attention. Moreover, current implementations of linear attention lack I/O-awareness and are thus slower than highly optimized implementations of softmax attention. This work describes a hardware-efficient algorithm for linear attention that trades off memory movement against parallelizability. The resulting implementation, dubbed FlashLinearAttention, is faster than FlashAttention-2 as a standalone layer even on short sequence lengths (e.g., 1K). We then generalize this algorithm to a more expressive variant of linear attention with data-dependent gates. When used as a replacement for the standard attention layer in Transformers, the resulting gated linear attention (GLA) Transformer is found to perform competitively against the LLaMA-architecture Transformer as well recent linear-time-inference baselines such as RetNet and Mamba on moderate-scale language modeling experiments. GLA Transformer is especially effective at length generalization, enabling a model trained on 2K to generalize to sequences longer than 20K without significant perplexity degradations. For training speed, the GLA Transformer has higher throughput than a similarly-sized Mamba model.}
}
@inproceedings{
ben-kish2025decimamba,
title={DeciMamba: Exploring the Length Extrapolation Potential of Mamba},
author={Assaf Ben-Kish and Itamar Zimerman and Shady Abu-Hussein and Nadav Cohen and Amir Globerson and Lior Wolf and Raja Giryes},
booktitle={The Thirteenth International Conference on Learning Representations},
year={2025},
url={https://openreview.net/forum?id=iWSl5Zyjjw}
}
@inproceedings{
ye2025longmamba,
title={LongMamba: Enhancing Mamba's Long-Context Capabilities via Training-Free Receptive Field Enlargement},
author={Zhifan Ye and Kejing Xia and Yonggan Fu and Xin Dong and Jihoon Hong and Xiangchi Yuan and Shizhe Diao and Jan Kautz and Pavlo Molchanov and Yingyan Celine Lin},
booktitle={The Thirteenth International Conference on Learning Representations},
year={2025},
url={https://openreview.net/forum?id=fMbLszVO1H}
}
@misc{waleffe2024empiricalstudymambabasedlanguage,
      title={An Empirical Study of Mamba-based Language Models}, 
      author={Roger Waleffe and Wonmin Byeon and Duncan Riach and Brandon Norick and Vijay Korthikanti and Tri Dao and Albert Gu and Ali Hatamizadeh and Sudhakar Singh and Deepak Narayanan and Garvit Kulshreshtha and Vartika Singh and Jared Casper and Jan Kautz and Mohammad Shoeybi and Bryan Catanzaro},
      year={2024},
      eprint={2406.07887},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2406.07887}, 
}
@inproceedings{
park2024can,
title={Can Mamba Learn How To Learn? A Comparative Study on In-Context Learning Tasks},
author={Jongho Park and Jaeseung Park and Zheyang Xiong and Nayoung Lee and Jaewoong Cho and Samet Oymak and Kangwook Lee and Dimitris Papailiopoulos},
booktitle={ICLR 2024 Workshop on Mathematical and Empirical Understanding of Foundation Models},
year={2024},
url={https://openreview.net/forum?id=xvr0Hctddy}
}
@misc{xu2025statespacemodelsstrong,
      title={State Space Models are Strong Text Rerankers}, 
      author={Zhichao Xu and Jinghua Yan and Ashim Gupta and Vivek Srikumar},
      year={2025},
      eprint={2412.14354},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.14354}, 
}
@misc{yuan2025remambaequipmambaeffective,
      title={ReMamba: Equip Mamba with Effective Long-Sequence Modeling}, 
      author={Danlong Yuan and Jiahao Liu and Bei Li and Huishuai Zhang and Jingang Wang and Xunliang Cai and Dongyan Zhao},
      year={2025},
      eprint={2408.15496},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2408.15496}, 
}
@misc{jelassi2024repeatmetransformersbetter,
      title={Repeat After Me: Transformers are Better than State Space Models at Copying}, 
      author={Samy Jelassi and David Brandfonbrener and Sham M. Kakade and Eran Malach},
      year={2024},
      eprint={2402.01032},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.01032}, 
}
@misc{arora2023zoologymeasuringimprovingrecall,
      title={Zoology: Measuring and Improving Recall in Efficient Language Models}, 
      author={Simran Arora and Sabri Eyuboglu and Aman Timalsina and Isys Johnson and Michael Poli and James Zou and Atri Rudra and Christopher Ré},
      year={2023},
      eprint={2312.04927},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2312.04927}, 
}
@misc{ren2024mambaenjoyfreelunch,
      title={Can Mamba Always Enjoy the "Free Lunch"?}, 
      author={Ruifeng Ren and Zhicong Li and Yong Liu},
      year={2024},
      eprint={2410.03810},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2410.03810}, 
}






@article{wang2022interpretability,
 author = {Wang, Kevin and Variengien, Alexandre and Conmy, Arthur and Shlegeris, Buck and Steinhardt, Jacob},
 journal = {arXiv preprint arXiv:2211.00593},
 title = {Interpretability in the wild: a circuit for indirect object identification in gpt-2 small},
 year = {2022}
}

@article{wang2023label,
 author = {Wang, Lean and Li, Lei and Dai, Damai and Chen, Deli and Zhou, Hao and Meng, Fandong and Zhou, Jie and Sun, Xu},
 journal = {arXiv preprint arXiv:2305.14160},
 title = {Label words are anchors: An information flow perspective for understanding in-context learning},
 year = {2023}
}

@article{wang2024buffer,
 author = {Wang, Zhiwei and Wang, Yunji and Zhang, Zhongwang and Zhou, Zhangchen and Jin, Hui and Hu, Tianyang and Sun, Jiacheng and Li, Zhenguo and Zhang, Yaoyu and Xu, Zhi-Qin John},
 journal = {arXiv preprint arXiv:2405.15302},
 title = {The Buffer Mechanism for Multi-Step Information Reasoning in Language Models},
 year = {2024}
}

@article{wang2024grokked,
 author = {Wang, Boshi and Yue, Xiang and Su, Yu and Sun, Huan},
 journal = {arXiv preprint arXiv:2405.15071},
 title = {Grokked Transformers are Implicit Reasoners: A Mechanistic Journey to the Edge of Generalization},
 year = {2024}
}

@article{wang2024improving,
 author = {Wang, Mingze and He, Haotian and Wang, Jinbo and Wang, Zilin and Huang, Guanhua and Xiong, Feiyu and Li, Zhiyu and Wu, Lei and others},
 journal = {arXiv preprint arXiv:2405.20763},
 title = {Improving Generalization and Convergence by Enhancing Implicit Regularization},
 year = {2024}
}

@article{wang2024understanding,
 author = {Wang, Mingze and Weinan, E},
 journal = {arXiv preprint arXiv:2402.00522},
 title = {Understanding the Expressive Power and Mechanisms of Transformer for Sequence Modeling},
 year = {2024}
}

@article{wang2024understanding1,
 author = {Wang, Xinyi and Amayuelas, Alfonso and Zhang, Kexun and Pan, Liangming and Chen, Wenhu and Wang, William Yang},
 journal = {arXiv preprint arXiv:2402.03268},
 title = {Understanding the reasoning ability of language models from the perspective of reasoning paths aggregation},
 year = {2024}
}

@article{wang2310theoretical,
 author = {Wang, Mingze and Wu, Lei},
 journal = {arXiv preprint arXiv:2310.00692},
 title = {A Theoretical Analysis of Noise Geometry in Stochastic Gradient Descent},
 year = {2023}
}

@article{wei2022chain,
 author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
 journal = {Advances in neural information processing systems},
 pages = {24824--24837},
 title = {Chain-of-thought prompting elicits reasoning in large language models},
 volume = {35},
 year = {2022}
}

@article{williams2019gradient,
 author = {Williams, Francis and Trager, Matthew and Panozzo, Daniele and Silva, Claudio and Zorin, Denis and Bruna, Joan},
 journal = {Advances in neural information processing systems},
 title = {Gradient dynamics of shallow univariate relu networks},
 volume = {32},
 year = {2019}
}

@article{williams_gradient_2019,
 archiveprefix = {arXiv},
 author = {Francis Williams and
Matthew Trager and
Cláudio T. Silva and
Daniele Panozzo and
Denis Zorin and
Joan Bruna},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/corr/abs-1906-07842.bib},
 eprint = {1906.07842},
 journal = {CoRR},
 timestamp = {Mon, 24 Jun 2019 17:28:45 +0200},
 title = {Gradient Dynamics of Shallow Univariate ReLU Networks},
 url = {http://arxiv.org/abs/1906.07842},
 volume = {abs/1906.07842},
 year = {2019}
}

@inproceedings{woodworth2020kernel,
 author = {Woodworth, Blake and Gunasekar, Suriya and Lee, Jason D and Moroshko, Edward and Savarese, Pedro and Golan, Itay and Soudry, Daniel and Srebro, Nathan},
 booktitle = {Conference on Learning Theory},
 organization = {PMLR},
 pages = {3635--3673},
 title = {Kernel and rich regimes in overparametrized models},
 year = {2020}
}

@article{wu2018sgd,
 author = {Wu, Lei and Ma, Chao and others},
 journal = {Advances in Neural Information Processing Systems},
 title = {How sgd selects the global minima in over-parameterized learning: A dynamical stability perspective},
 volume = {31},
 year = {2018}
}

@inproceedings{wu2023implicit,
 author = {Wu, Lei and Su, Weijie J},
 booktitle = {International Conference on Machine Learning},
 organization = {PMLR},
 pages = {37656--37684},
 title = {The implicit regularization of dynamical stability in stochastic gradient descent},
 year = {2023}
}

@inproceedings{xiong2020layer,
 author = {Xiong, Ruibin and Yang, Yunchang and He, Di and Zheng, Kai and Zheng, Shuxin and Xing, Chen and Zhang, Huishuai and Lan, Yanyan and Wang, Liwei and Liu, Tieyan},
 booktitle = {International Conference on Machine Learning},
 organization = {PMLR},
 pages = {10524--10533},
 title = {On layer normalization in the transformer architecture},
 year = {2020}
}

@article{xu2019frequency,
 author = {Xu, Zhi-Qin John and Zhang, Yaoyu and Luo, Tao and Xiao, Yanyang and Ma, Zheng},
 journal = {arXiv preprint arXiv:1901.06523},
 title = {Frequency principle: Fourier analysis sheds light on deep neural networks},
 year = {2019}
}

@article{xu2019understanding,
 author = {Xu, Jingjing and Sun, Xu and Zhang, Zhiyuan and Zhao, Guangxiang and Lin, Junyang},
 journal = {Advances in neural information processing systems},
 title = {Understanding and improving layer normalization},
 volume = {32},
 year = {2019}
}

@article{xu2021towards,
 author = {Xu, Zhi-Qin John and Zhou, Hanxu and Luo, Tao and Zhang, Yaoyu},
 journal = {arXiv preprint arXiv:2105.11686},
 title = {Towards understanding the condensation of two-layer neural networks at initial training},
 volume = {2},
 year = {2021}
}

@article{yang2024large,
 author = {Yang, Sohee and Gribovskaya, Elena and Kassner, Nora and Geva, Mor and Riedel, Sebastian},
 journal = {arXiv preprint arXiv:2402.16837},
 title = {Do Large Language Models Latently Perform Multi-Hop Reasoning?},
 year = {2024}
}

@article{yao2024tree,
 author = {Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Tom and Cao, Yuan and Narasimhan, Karthik},
 journal = {Advances in Neural Information Processing Systems},
 title = {Tree of thoughts: Deliberate problem solving with large language models},
 volume = {36},
 year = {2024}
}

@article{yao2025analysis, 
 author = {Junjie Yao and Zhongwang Zhang and Zhi-Qin John Xu},
 journal = {Forty-second International Conference on Machine Learning}, 
 title = {An Analysis for Reasoning Bias of Language Models with Small Initialization}, 
 year = {2025}
}

@misc{yin2025panguultrapushinglimits,
 archiveprefix = {arXiv},
 author = {Yichun Yin and Wenyong Huang and Kaikai Song and Yehui Tang and Xueyu Wu and Wei Guo and Peng Guo and Yaoyuan Wang and Xiaojun Meng and Yasheng Wang and Dong Li and Can Chen and Dandan Tu and Yin Li and Fisher Yu and Ruiming Tang and Yunhe Wang and Baojun Wang and Bin Wang and Bo Wang and Boxiao Liu and Changzheng Zhang and Duyu Tang and Fei Mi and Hui Jin and Jiansheng Wei and Jiarui Qin and Jinpeng Li and Jun Zhao and Liqun Deng and Lin Li and Minghui Xu and Naifu Zhang and Nianzu Zheng and Qiang Li and Rongju Ruan and Shengjun Cheng and Tianyu Guo and Wei He and Wei Li and Weiwen Liu and Wulong Liu and Xinyi Dai and Yonghan Dong and Yu Pan and Yue Li and Yufei Wang and Yujun Li and Yunsheng Ni and Zhe Liu and Zhenhe Zhang and Zhicheng Liu},
 eprint = {2504.07866},
 primaryclass = {cs.CL},
 title = {Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs},
 url = {https://arxiv.org/abs/2504.07866},
 year = {2025}
}

@inproceedings{zhang-etal-2019-improving,
 abstract = {The general trend in NLP is towards increasing model capacity and performance via deeper neural networks. However, simply stacking more layers of the popular Transformer architecture for machine translation results in poor convergence and high computational overhead. Our empirical analysis suggests that convergence is poor due to gradient vanishing caused by the interaction between residual connection and layer normalization. We propose depth-scaled initialization (DS-Init), which decreases parameter variance at the initialization stage, and reduces output variance of residual connections so as to ease gradient back-propagation through normalization layers. To address computational cost, we propose a merged attention sublayer (MAtt) which combines a simplified average-based self-attention sublayer and the encoder-decoder attention sublayer on the decoder side. Results on WMT and IWSLT translation tasks with five translation directions show that deep Transformers with DS-Init and MAtt can substantially outperform their base counterpart in terms of BLEU (+1.1 BLEU on average for 12-layer models), while matching the decoding speed of the baseline model thanks to the efficiency improvements of MAtt. Source code for reproduction will be released soon.},
 address = {Hong Kong, China},
 author = {Zhang, Biao  and
Titov, Ivan  and
Sennrich, Rico},
 booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
 doi = {10.18653/v1/D19-1083},
 editor = {Inui, Kentaro  and
Jiang, Jing  and
Ng, Vincent  and
Wan, Xiaojun},
 month = {November},
 pages = {898--909},
 publisher = {Association for Computational Linguistics},
 title = {Improving Deep Transformer with Depth-Scaled Initialization and Merged Attention},
 url = {https://aclanthology.org/D19-1083/},
 year = {2019}
}

@inproceedings{zhang2020type,
 author = {Zhang, Yaoyu and Xu, Zhi-Qin John and Luo, Tao and Ma, Zheng},
 booktitle = {Mathematical and Scientific Machine Learning},
 organization = {PMLR},
 pages = {144--164},
 title = {A type of generalization error induced by initialization in deep neural networks},
 year = {2020}
}

@article{zhang2021embedding,
 author = {Zhang, Yaoyu and Li, Yuqing and Zhang, Zhongwang and Luo, Tao and Xu, Zhi-Qin John},
 journal = {arXiv preprint arXiv:2111.15527},
 title = {Embedding principle: a hierarchical structure of loss landscape of deep neural networks},
 year = {2021}
}

@article{zhang2021embeddingnips,
 author = {Zhang, Yaoyu and Zhang, Zhongwang and Luo, Tao and Xu, Zhiqin J},
 journal = {Advances in Neural Information Processing Systems},
 pages = {14848--14859},
 title = {Embedding principle of loss landscape of deep neural networks},
 volume = {34},
 year = {2021}
}

@article{zhang2021pointer,
 author = {Zhang, Chiyuan and Raghu, Maithra and Kleinberg, Jon and Bengio, Samy},
 journal = {arXiv preprint arXiv:2107.12580},
 title = {Pointer value retrieval: A new benchmark for understanding the limits of neural network generalization},
 year = {2021}
}

@article{zhang2021understanding,
 author = {Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
 journal = {Communications of the ACM},
 number = {3},
 pages = {107--115},
 publisher = {ACM New York, NY, USA},
 title = {Understanding deep learning (still) requires rethinking generalization},
 volume = {64},
 year = {2021}
}

@article{zhang2023beam,
 author = {Zhang, Jiahao and Zhang, Haiyang and Zhang, Dongmei and Liu, Yong and Huang, Shen},
 journal = {arXiv preprint arXiv:2308.08973},
 title = {Beam retrieval: General end-to-end retrieval for multi-hop question answering},
 year = {2023}
}

@article{zhang2023stochastic,
 author = {Zhang, Zhongwang and Li, Yuqing and Luo, Tao and Xu, Zhi-Qin John},
 journal = {arXiv preprint arXiv:2305.15850},
 title = {Stochastic Modified Equations and Dynamics of Dropout Algorithm},
 year = {2023}
}

@article{zhang2024anchor,
 author = {Zhang, Zhongwang and Wang, Zhiwei and Yao, Junjie and Zhou, Zhangchen and Li, Xiaolong and Xu, Zhi-Qin John and others},
 journal = {arXiv preprint arXiv:2401.08309},
 title = {Anchor function: a type of benchmark functions for studying language models},
 year = {2024}
}

@article{zhang2024diagram,
 author = {Zhang, Yifan and Yuan, Yang and Yao, Andrew Chi-Chih},
 journal = {arXiv preprint arXiv:2409.10038},
 title = {On the Diagram of Thought},
 year = {2024}
}

@article{zhang2024implicit,
 author = {Zhang, Zhongwang and Xu, Zhi-Qin John},
 journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
 publisher = {IEEE},
 title = {Implicit regularization of dropout},
 year = {2024}
}

@article{zhang2024initialization,
 author = {Zhang, Zhongwang and Lin, Pengxiao and Wang, Zhiwei and Zhang, Yaoyu and Xu, Zhi-Qin John},
 journal = {arXiv preprint arXiv:2405.05409},
 title = {Initialization is Critical to Whether Transformers Fit Composite Functions by Inference or Memorizing},
 year = {2024}
}

@article{zhang2024trained,
 author = {Zhang, Ruiqi and Frei, Spencer and Bartlett, Peter L},
 journal = {Journal of Machine Learning Research},
 number = {49},
 pages = {1--55},
 title = {Trained transformers learn linear models in-context},
 volume = {25},
 year = {2024}
}

@article{zhang2025complexity,
 author = {Zhang, Zhongwang and Lin, Pengxiao and Wang, Zhiwei and Zhang, Yaoyu and Xu, Zhi-Qin John},
 journal = {arXiv preprint arXiv:2501.08537},
 title = {Complexity Control Facilitates Reasoning-Based Compositional Generalization in Transformers},
 year = {2025}
}

@misc{zhang2025trainingdynamicsincontextlearning,
 archiveprefix = {arXiv},
 author = {Yedi Zhang and Aaditya K. Singh and Peter E. Latham and Andrew Saxe},
 eprint = {2501.16265},
 primaryclass = {cs.LG},
 title = {Training Dynamics of In-Context Learning in Linear Attention},
 url = {https://arxiv.org/abs/2501.16265},
 year = {2025}
}

@article{zhang_type_2019,
 author = {Zhang, Yaoyu and Xu, Zhi-Qin John and Luo, Tao and Ma, Zheng},
 journal = {arXiv:1905.07777 [cs, stat]},
 title = {A type of generalization error induced by initialization in deep neural networks},
 year = {2019}
}

@misc{zhangzhongwang,
 archiveprefix = {arXiv},
 author = {Zhongwang Zhang and Pengxiao Lin and Zhiwei Wang and Yaoyu Zhang and Zhi-Qin John Xu},
 eprint = {2405.05409},
 primaryclass = {cs.LG},
 title = {Initialization is Critical to Whether Transformers Fit Composite Functions by Inference or Memorizing},
 url = {https://arxiv.org/abs/2405.05409},
 year = {2024}
}

@article{zhou2022empirical,
 author = {Zhou, Hanxu and Qixuan, Zhou and Jin, Zhenyuan and Luo, Tao and Zhang, Yaoyu and Xu, Zhi-Qin},
 journal = {Advances in Neural Information Processing Systems},
 pages = {26021--26033},
 title = {Empirical phase diagram for three-layer neural networks with infinite width},
 volume = {35},
 year = {2022}
}

@article{zhou2022towards,
 author = {Zhou, Hanxu and Qixuan, Zhou and Luo, Tao and Zhang, Yaoyu and Xu, Zhi-Qin},
 journal = {Advances in Neural Information Processing Systems},
 pages = {2184--2196},
 title = {Towards understanding the condensation of neural networks at initial training},
 volume = {35},
 year = {2022}
}

@article{zhou2023understanding,
 author = {Zhou, Zhangchen and Zhou, Hanxu and Li, Yuqing and Xu, Zhi-Qin John},
 journal = {arXiv preprint arXiv:2305.09947},
 title = {Understanding the initial condensation of convolutional neural networks},
 year = {2023}
}

@article{zhu2018anisotropic,
 author = {Zhu, Zhanxing and Wu, Jingfeng and Yu, Bing and Wu, Lei and Ma, Jinwen},
 journal = {arXiv preprint arXiv:1803.00195},
 title = {The anisotropic noise in stochastic gradient descent: Its behavior of escaping from sharp minima and regularization effects},
 year = {2018}
}

@article{zhu2021gradinit,
 author = {Zhu, Chen and Ni, Renkun and Xu, Zheng and Kong, Kezhi and Huang, W Ronny and Goldstein, Tom},
 journal = {Advances in Neural Information Processing Systems},
 pages = {16410--16422},
 title = {Gradinit: Learning to initialize neural networks for stable and efficient training},
 volume = {34},
 year = {2021}
}


@article{o2015introduction,
  title={An introduction to convolutional neural networks},
  author={O'shea, Keiron and Nash, Ryan},
  journal={arXiv preprint arXiv:1511.08458},
  year={2015}
}